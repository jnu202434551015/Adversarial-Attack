# Adversarial-Attack
Adversarial-Attack-Deepfake

* Adversarial Deepfakes: Evaluating Vulnerability of Deepfake Detectors to Adversarial Examples [[PDF]](http://openaccess.thecvf.com/content/WACV2021/papers/Hussain_Adversarial_Deepfakes_Evaluating_Vulnerability_of_Deepfake_Detectors_to_Adversarial_Examples_WACV_2021_paper.pdf)
* [Hussain, S., Neekhara, P., Jere, M., Koushanfar, F., & McAuley, J. (2021). Adversarial deepfakes: Evaluating vulnerability of deepfake detectors to adversarial examples. In Proceedings of the IEEE/CVF winter conference on applications of computer vision (pp. 3348-3357).]
* Thinking Racial Bias in Fair Forgery Detection: Models, Datasets and Evaluations [[PDF]](https://ojs.aaai.org/index.php/AAAI/article/view/32572/34727)
* [Liu, D., Wang, Z., Peng, C., Wang, N., Hu, R., & Gao, X. (2025, April). Thinking racial bias in fair forgery detection: Models, datasets and evaluations. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 39, No. 5, pp. 5379-5387).]
* AI-Face: A Million-Scale Demographically Annotated AI-Generated Face Dataset and Fairness Benchmark [[PDF]](https://openaccess.thecvf.com/content/CVPR2025/papers/Lin_AI-Face_A_Million-Scale_Demographically_Annotated_AI-Generated_Face_Dataset_and_Fairness_CVPR_2025_paper.pdf) [[code]](https://github.com/Purdue-M2/AI-Face-FairnessBench)
* [Lin, L., Santosh, S., Wu, M., Wang, X., & Hu, S. (2025). Ai-face: A million-scale demographically annotated ai-generated face dataset and fairness benchmark. In Proceedings of the Computer Vision and Pattern Recognition Conference (pp. 3503-3515).]

## Fairness-enhanced Model
* Improving Fairness in Deepfake Detection [[PDF]](https://openaccess.thecvf.com/content/WACV2024/papers/Ju_Improving_Fairness_in_Deepfake_Detection_WACV_2024_paper.pdf)
* [Ju, Y., Hu, S., Jia, S., Chen, G. H., & Lyu, S. (2024). Improving fairness in deepfake detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 4655-4665).]
* Preserving Fairness Generalization in Deepfake Detection [[PDF]](https://openaccess.thecvf.com/content/CVPR2024/papers/Lin_Preserving_Fairness_Generalization_in_Deepfake_Detection_CVPR_2024_paper.pdf) [[code]](https://github.com/Purdue-M2/Fairness-Generalization)
* [Lin, L., He, X., Ju, Y., Wang, X., Ding, F., & Hu, S. (2024). Preserving fairness generalization in deepfake detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 16815-16825).]

## Attack - Deepfake
* Spectrum Translation for Refinement of Image Generation (STIG) Based on Contrastive Learning and Spectral Filter Profile [[PDF]](https://ojs.aaai.org/index.php/AAAI/article/view/28074/28154)
* [Lee, S., Jung, S. W., & Seo, H. (2024, March). Spectrum translation for refinement of image generation (STIG) based on contrastive learning and spectral filter profile. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 38, No. 4, pp. 2929-2937).]
* Evading Forensic Classifiers with Attribute-Conditioned Adversarial Faces [[PDF]](https://openaccess.thecvf.com/content/CVPR2023/papers/Shamshad_Evading_Forensic_Classifiers_With_Attribute-Conditioned_Adversarial_Faces_CVPR_2023_paper.pdf)
* [Shamshad, F., Srivatsan, K., & Nandakumar, K. (2023). Evading forensic classifiers with attribute-conditioned adversarial faces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 16469-16478).]
* AVA: Inconspicuous Attribute Variation-based Adversarial Attack bypassing DeepFake Detection [[PDF]](https://arxiv.org/pdf/2312.08675)[[code]](https://github.com/AnonymousUserA/AVA)
* [Meng, X., Wang, L., Guo, S., Ju, L., & Zhao, Q. (2024, May). Ava: Inconspicuous attribute variation-based adversarial attack bypassing deepfake detection. In 2024 IEEE Symposium on Security and Privacy (SP) (pp. 74-90). IEEE.]
* StealthDiffusion: Towards Evading Diffusion Forensic Detection through Diffusion Model [[PDF]](https://dl.acm.org/doi/pdf/10.1145/3664647.3681535?casa_token=IUs-9nw5zgoAAAAA:CfVywyX1FOauGKvwjfRiV9TzhPLSVOj5ySXaUklt8y3h8zwZIoMHA-oxHnjLeoeZzUL_EtFIAGfOww)[[code]](https://github.com/wyczzy/StealthDiffusion)
* [Zhou, Z., Sun, K., Chen, Z., Kuang, H., Sun, X., & Ji, R. (2024, October). Stealthdiffusion: Towards evading diffusion forensic detection through diffusion model. In Proceedings of the 32nd ACM International Conference on Multimedia (pp. 3627-3636).]
* Evading DeepFake Detectors via Adversarial Statistical Consistency [[PDF]](http://openaccess.thecvf.com/content/CVPR2023/papers/Hou_Evading_DeepFake_Detectors_via_Adversarial_Statistical_Consistency_CVPR_2023_paper.pdf)[[code]](https://github.com/tobuta/evadingfakedetector)
* [Hou, Y., Guo, Q., Huang, Y., Xie, X., Ma, L., & Zhao, J. (2023). Evading deepfake detectors via adversarial statistical consistency. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 12271-12280).]
* Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering [[PDF]](https://arxiv.org/pdf/2009.09213)[[code]](https://github.com/fanoflck/Implicit-spatial-notch)
* [Huang, Y., Juefei-Xu, F., Guo, Q., Liu, Y., & Pu, G. (2023). Dodging deepfake detection via implicit spatial-domain notch filtering. IEEE Transactions on Circuits and Systems for Video Technology, 34(8), 6949-6962.]
* Making DeepFakes more spurious: evading deep face forgery detection via trace removal attack [[PDF]](https://arxiv.org/pdf/2203.11433)
* [Liu, C., Chen, H., Zhu, T., Zhang, J., & Zhou, W. (2023). Making DeepFakes more spurious: evading deep face forgery detection via trace removal attack. IEEE Transactions on Dependable and Secure Computing, 20(6), 5182-5196.]
* Benchmarking Adversarial Robustness on Image Classification [[PDF]](http://openaccess.thecvf.com/content_CVPR_2020/papers/Dong_Benchmarking_Adversarial_Robustness_on_Image_Classification_CVPR_2020_paper.pdf)[[code]](https://github.com/thu-ml/ares)
* [Dong, Y., Fu, Q. A., Yang, X., Pang, T., Su, H., Xiao, Z., & Zhu, J. (2020). Benchmarking adversarial robustness on image classification. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 321-331).]

## Attack - Other
* Non-Adaptive Adversarial Face Generation [[PDF]](https://arxiv.org/pdf/2507.12107?)
* [NIPS 2025]
* TraceEvader: Making DeepFakes More Untraceable via Evading the Forgery Model Attribution [[PDF]](https://ojs.aaai.org/index.php/AAAI/article/download/29973/31705)
* [Wu, M., Ma, J., Wang, R., Zhang, S., Liang, Z., Li, B., ... & Wang, L. (2024, March). Traceevader: Making deepfakes more untraceable via evading the forgery model attribution. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 38, No. 18, pp. 19965-19973).]
* Rethinking Impersonation and Dodging Attacks on Face Recognition Systems [[PDF]](https://dl.acm.org/doi/pdf/10.1145/3664647.3681440?casa_token=EFpWg1Hq8fwAAAAA:VJY953Tt7mTSGD3SOR5zUkje8gJCzkUnAMIogTmZOf0ZPnov46pfcg57ZbTbWE1LBa7evnONgHiVVQ)
* [Zhou, F., Zhou, Q., Yin, B., Zheng, H., Lu, X., Ma, L., & Ling, H. (2024, October). Rethinking impersonation and dodging attacks on face recognition systems. In Proceedings of the 32nd ACM International Conference on Multimedia (pp. 2487-2496).]
* Adversarial Attention Perturbations for Large Object Detection Transformers [[PDF]](https://openaccess.thecvf.com/content/ICCV2025/papers/Yahn_Adversarial_Attention_Perturbations_for_Large_Object_Detection_Transformers_ICCV_2025_paper.pdf)
* [Yahn, Z., Tekin, S. F., Ilhan, F., Hu, S., Huang, T., Xu, Y., ... & Liu, L. (2025). Adversarial Attention Perturbations for Large Object Detection Transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 3184-3193).]

## Sample
* Influence-Based Fair Selection for Sample-Discriminative Backdoor Attack [[PDF]](https://ojs.aaai.org/index.php/AAAI/article/view/35449/37604)
* [Wei, Q., He, S., Zhang, J., Feng, L., & An, B. (2025, April). Influence-Based Fair Selection for Sample-Discriminative Backdoor Attack. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 39, No. 20, pp. 21474-21481).]
* Revisiting model fairness via adversarial examples [[PDF]](https://www.sciencedirect.com/science/article/pii/S0950705123005270?casa_token=Rg0mPO5A32EAAAAA:pdnjwnuRjF_H3C2hw0WMzbjSvbuW8CGzjisHNbHrPU8UrPTNRQR8gm_B87N7LTc83dZVB7vr_CA) [[code]](https://github.com/TaocsZhang/Fairness-Attack-via-Adversarial-Examples)
* [Zhang, T., Zhu, T., Li, J., Zhou, W., & Yu, P. S. (2023). Revisiting model fairness via adversarial examples. Knowledge-Based Systems, 277, 110777.]
